{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import List, Tuple, Union, Dict\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Users/vratinsrivastava/Desktop/research/graphpocket/gnn')\n",
    "\n",
    "from dataloader import get_dataloader, create_dataset\n",
    "from graphpocket import GraphPocket\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gvp\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def _norm_no_nan(x, axis=-1, keepdims=False, eps=1e-8, sqrt=True):\n",
    "    '''\n",
    "    L2 norm of tensor clamped above a minimum value `eps`.\n",
    "    \n",
    "    :param sqrt: if `False`, returns the square of the L2 norm\n",
    "    '''\n",
    "    out = torch.clamp(torch.sum(torch.square(x), axis, keepdims), min=eps)\n",
    "    return torch.sqrt(out) if sqrt else out\n",
    "\n",
    "def _rbf(D, D_min=0., D_max=20., D_count=16):\n",
    "    '''\n",
    "    From https://github.com/jingraham/neurips19-graph-protein-design\n",
    "    \n",
    "    Returns an RBF embedding of `torch.Tensor` `D` along a new axis=-1.\n",
    "    That is, if `D` has shape [...dims], then the returned tensor will have\n",
    "    shape [...dims, D_count].\n",
    "    '''\n",
    "    device = D.device\n",
    "    D_mu = torch.linspace(D_min, D_max, D_count, device=device)\n",
    "    D_mu = D_mu.view([1, -1])\n",
    "    D_sigma = (D_max - D_min) / D_count\n",
    "    D_expand = torch.unsqueeze(D, -1)\n",
    "\n",
    "    RBF = torch.exp(-((D_expand - D_mu) / D_sigma) ** 2)\n",
    "    return RBF\n",
    "\n",
    "class GVP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_vectors_in,\n",
    "        dim_vectors_out,\n",
    "        dim_feats_in,\n",
    "        dim_feats_out,\n",
    "        hidden_vectors = None,\n",
    "        feats_activation = nn.SiLU(),\n",
    "        vectors_activation = nn.Sigmoid(),\n",
    "        vector_gating = True,\n",
    "        xavier_init = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim_vectors_in = dim_vectors_in\n",
    "        self.dim_feats_in = dim_feats_in\n",
    "\n",
    "        self.dim_vectors_out = dim_vectors_out\n",
    "        dim_h = max(dim_vectors_in, dim_vectors_out) if hidden_vectors is None else hidden_vectors\n",
    "\n",
    "        # create Wh and Wu matricies\n",
    "        wh_k = 1/math.sqrt(dim_vectors_in)\n",
    "        wu_k = 1/math.sqrt(dim_h)\n",
    "        self.Wh = torch.zeros(dim_vectors_in, dim_h, dtype=torch.float32).uniform_(-wh_k, wh_k)\n",
    "        self.Wu = torch.zeros(dim_h, dim_vectors_out, dtype=torch.float32).uniform_(-wu_k, wu_k)\n",
    "        self.Wh = nn.Parameter(self.Wh)\n",
    "        self.Wu = nn.Parameter(self.Wu)\n",
    "\n",
    "        self.vectors_activation = vectors_activation\n",
    "\n",
    "        self.to_feats_out = nn.Sequential(\n",
    "            nn.Linear(dim_h + dim_feats_in, dim_feats_out),\n",
    "            feats_activation\n",
    "        )\n",
    "\n",
    "        # branching logic to use old GVP, or GVP with vector gating\n",
    "        if vector_gating:\n",
    "            self.scalar_to_vector_gates = nn.Linear(dim_feats_out, dim_vectors_out)\n",
    "            if xavier_init:\n",
    "                nn.init.xavier_uniform_(self.scalar_to_vector_gates.weight, gain=1)\n",
    "                nn.init.constant_(self.scalar_to_vector_gates.bias, 0)\n",
    "        else:\n",
    "            self.scalar_to_vector_gates = None\n",
    "\n",
    "    def forward(self, data):\n",
    "        feats, vectors = data\n",
    "        b, n, _, v, c  = *feats.shape, *vectors.shape\n",
    "\n",
    "        assert c == 3 and v == self.dim_vectors_in, 'vectors have wrong dimensions'\n",
    "        assert n == self.dim_feats_in, 'scalar features have wrong dimensions'\n",
    "\n",
    "        Vh = einsum('b v c, v h -> b h c', vectors, self.Wh)\n",
    "        Vu = einsum('b h c, h u -> b u c', Vh, self.Wu)\n",
    "\n",
    "        sh = _norm_no_nan(Vh)\n",
    "\n",
    "        s = torch.cat((feats, sh), dim = 1)\n",
    "\n",
    "        feats_out = self.to_feats_out(s)\n",
    "\n",
    "        if exists(self.scalar_to_vector_gates):\n",
    "            gating = self.scalar_to_vector_gates(feats_out)\n",
    "            gating = gating.unsqueeze(dim = -1)\n",
    "        else:\n",
    "            gating = _norm_no_nan(Vu)\n",
    "\n",
    "        vectors_out = self.vectors_activation(gating) * Vu\n",
    "\n",
    "        # if torch.isnan(feats_out).any() or torch.isnan(vectors_out).any():\n",
    "        #     raise ValueError(\"NaNs in GVP forward pass\")\n",
    "\n",
    "        return (feats_out, vectors_out)\n",
    "    \n",
    "class _VDropout(nn.Module):\n",
    "    '''\n",
    "    Vector channel dropout where the elements of each\n",
    "    vector channel are dropped together.\n",
    "    '''\n",
    "    def __init__(self, drop_rate):\n",
    "        super(_VDropout, self).__init__()\n",
    "        self.drop_rate = drop_rate\n",
    "        self.dummy_param = nn.Parameter(torch.empty(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        :param x: `torch.Tensor` corresponding to vector channels\n",
    "        '''\n",
    "        device = self.dummy_param.device\n",
    "        if not self.training:\n",
    "            return x\n",
    "        mask = torch.bernoulli(\n",
    "            (1 - self.drop_rate) * torch.ones(x.shape[:-1], device=device)\n",
    "        ).unsqueeze(-1)\n",
    "        x = mask * x / (1 - self.drop_rate)\n",
    "        return x\n",
    "    \n",
    "class GVPDropout(nn.Module):\n",
    "    \"\"\" Separate dropout for scalars and vectors. \"\"\"\n",
    "    def __init__(self, rate):\n",
    "        super().__init__()\n",
    "        self.vector_dropout = _VDropout(rate)\n",
    "        self.feat_dropout = nn.Dropout(rate)\n",
    "\n",
    "    def forward(self, feats, vectors):\n",
    "        return self.feat_dropout(feats), self.vector_dropout(vectors)\n",
    "\n",
    "\n",
    "class GVPLayerNorm(nn.Module):\n",
    "    \"\"\" Normal layer norm for scalars, nontrainable norm for vectors. \"\"\"\n",
    "    def __init__(self, feats_h_size, eps = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.feat_norm = nn.LayerNorm(feats_h_size)\n",
    "\n",
    "    def forward(self, feats, vectors):\n",
    "\n",
    "        normed_feats = self.feat_norm(feats)\n",
    "\n",
    "        vn = _norm_no_nan(vectors, axis=-1, keepdims=True, sqrt=False)\n",
    "        vn = torch.sqrt(torch.mean(vn, dim=-2, keepdim=True) + self.eps ) + self.eps\n",
    "        normed_vectors = vectors / vn\n",
    "        return normed_feats, normed_vectors\n",
    "    \n",
    "#convolution\n",
    "class GVPEdgeConv(nn.Module):\n",
    "\n",
    "    def __init__(self, scalar_size: int = 128, vector_size: int = 16,\n",
    "                  scalar_activation=nn.SiLU, vector_activation=nn.Sigmoid,\n",
    "                  n_message_gvps: int = 1, n_update_gvps: int = 1,\n",
    "                  rbf_dmax: float = 3.5, rbf_dim: int = 16,\n",
    "                  edge_feat_size: int = 0, message_norm: Union[float, str] = 4, dropout: float = 0.0,):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.scalar_size = scalar_size\n",
    "        self.vector_size = vector_size\n",
    "        self.scalar_activation = scalar_activation\n",
    "        self.vector_activation = vector_activation\n",
    "        self.n_message_gvps = n_message_gvps\n",
    "        self.n_update_gvps = n_update_gvps\n",
    "        self.edge_feat_size = edge_feat_size\n",
    "        self.rbf_dmax = rbf_dmax\n",
    "        self.rbf_dim = rbf_dim\n",
    "        self.dropout_rate = dropout\n",
    "        self.message_norm = message_norm\n",
    "\n",
    "        # create message passing function\n",
    "        message_gvps = []\n",
    "        for i in range(n_message_gvps):\n",
    "\n",
    "            dim_vectors_in = vector_size\n",
    "            dim_feats_in = scalar_size\n",
    "\n",
    "            # on the first layer, there is an extra edge vector for the displacement vector between the two node positions\n",
    "            if i == 0:\n",
    "                dim_vectors_in += 1\n",
    "                dim_feats_in += rbf_dim\n",
    "\n",
    "            message_gvps.append(\n",
    "                GVP(dim_vectors_in=dim_vectors_in, \n",
    "                    dim_vectors_out=vector_size, \n",
    "                    dim_feats_in=dim_feats_in, \n",
    "                    dim_feats_out=scalar_size, \n",
    "                    feats_activation=scalar_activation(), \n",
    "                    vectors_activation=vector_activation(), \n",
    "                    vector_gating=True)\n",
    "            )\n",
    "        self.edge_message = nn.Sequential(*message_gvps)\n",
    "\n",
    "        # create update function\n",
    "        update_gvps = []\n",
    "        for i in range(n_update_gvps):\n",
    "            update_gvps.append(\n",
    "                GVP(dim_vectors_in=vector_size, \n",
    "                    dim_vectors_out=vector_size, \n",
    "                    dim_feats_in=scalar_size, \n",
    "                    dim_feats_out=scalar_size, \n",
    "                    feats_activation=scalar_activation(), \n",
    "                    vectors_activation=vector_activation(), \n",
    "                    vector_gating=True)\n",
    "            )\n",
    "        self.node_update = nn.Sequential(*update_gvps)\n",
    "\n",
    "        \n",
    "        self.dropout = GVPDropout(self.dropout_rate)\n",
    "        self.message_layer_norm = GVPLayerNorm(self.scalar_size)\n",
    "        self.update_layer_norm = GVPLayerNorm(self.scalar_size)\n",
    "\n",
    "        if self.message_norm == 'mean':\n",
    "            self.agg_func = fn.mean\n",
    "        else:\n",
    "            self.agg_func = fn.sum\n",
    "\n",
    "    def forward(self, g: dgl.DGLHeteroGraph, \n",
    "                rec_feats: Tuple[torch.Tensor, torch.Tensor, torch.Tensor], \n",
    "                edge_feats: torch.Tensor = None, \n",
    "                z: Union[float, torch.Tensor] = 1):\n",
    "        # vec_feat has shape (n_nodes, n_vectors, 3)\n",
    "\n",
    "        with g.local_scope():\n",
    "\n",
    "            scalar_feat, coord_feat, vec_feat = rec_feats\n",
    "            g.ndata[\"h\"] = scalar_feat\n",
    "            g.ndata[\"x\"] = coord_feat\n",
    "            g.ndata[\"v\"] = vec_feat\n",
    "            \n",
    "            # edge feature\n",
    "            if self.edge_feat_size > 0:\n",
    "                assert edge_feats is not None, \"Edge features must be provided.\"\n",
    "                g.edata[\"a\"] = edge_feats\n",
    "\n",
    "            # get vectors between node positions\n",
    "            g.apply_edges(fn.u_sub_v(\"x\", \"x\", \"x_diff\"))\n",
    "\n",
    "            # normalize x_diff and compute rbf embedding of edge distance\n",
    "            # dij = torch.norm(g.edges[self.edge_type].data['x_diff'], dim=-1, keepdim=True)\n",
    "            dij = _norm_no_nan(g.edata['x_diff'], keepdims=True) + 1e-8\n",
    "            g.edata['x_diff'] = g.edata['x_diff'] / dij\n",
    "            g.edata['d'] = _rbf(dij.squeeze(1), D_max=self.rbf_dmax, D_count=self.rbf_dim)\n",
    "\n",
    "            # compute messages on every edge\n",
    "            g.apply_edges(self.message)\n",
    "\n",
    "            # aggregate messages from every edge\n",
    "            g.update_all(fn.copy_e(\"scalar_msg\", \"m\"), self.agg_func(\"m\", \"scalar_msg\"))\n",
    "            g.update_all(fn.copy_e(\"vec_msg\", \"m\"), self.agg_func(\"m\", \"vec_msg\"))\n",
    "\n",
    "\n",
    "            # get aggregated scalar and vector messages\n",
    "            scalar_msg = g.ndata[\"scalar_msg\"] / z\n",
    "            if isinstance(z, torch.Tensor):\n",
    "                z = z.unsqueeze(-1)\n",
    "            vec_msg = g.ndata[\"vec_msg\"] / z\n",
    "\n",
    "            # dropout scalar and vector messages\n",
    "            scalar_msg, vec_msg = self.dropout(scalar_msg, vec_msg)\n",
    "\n",
    "            # update scalar and vector features, apply layernorm\n",
    "            scalar_feat = g.ndata['h'] + scalar_msg\n",
    "            vec_feat = g.ndata['v'] + vec_msg\n",
    "            scalar_feat, vec_feat = self.message_layer_norm(scalar_feat, vec_feat)\n",
    "\n",
    "            # apply node update function, apply dropout to residuals, apply layernorm\n",
    "            scalar_residual, vec_residual = self.node_update((scalar_feat, vec_feat))\n",
    "            scalar_residual, vec_residual = self.dropout(scalar_residual, vec_residual)\n",
    "            scalar_feat = scalar_feat + scalar_residual\n",
    "            vec_feat = vec_feat + vec_residual\n",
    "            scalar_feat, vec_feat = self.update_layer_norm(scalar_feat, vec_feat)\n",
    "\n",
    "        return scalar_feat, vec_feat\n",
    "\n",
    "    def message(self, edges):\n",
    "\n",
    "        vec_feats = torch.cat([edges.data[\"x_diff\"].unsqueeze(1), edges.src[\"v\"]], dim=1)\n",
    "\n",
    "        # create scalar features\n",
    "        scalar_feats = [ edges.src['h'], edges.data['d'] ]\n",
    "\n",
    "        # if self.edge_feat_size > 0:\n",
    "        #     scalar_feats.append(edges.data['a'])\n",
    "\n",
    "        scalar_feats = torch.cat(scalar_feats, dim=1)\n",
    "\n",
    "        scalar_message, vector_message = self.edge_message((scalar_feats, vec_feats))\n",
    "\n",
    "        return {\"scalar_msg\": scalar_message, \"vec_msg\": vector_message}\n",
    "\n",
    "    \n",
    "class ReceptorEncoderGVP(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 in_scalar_size: int, \n",
    "                 out_scalar_size: int = 128, \n",
    "                 n_message_gvps: int = 1,\n",
    "                 n_update_gvps: int = 1,\n",
    "                 vector_size: int = 16,\n",
    "                 n_convs: int = 3, \n",
    "                 message_norm: Union[float, str] = 10, \n",
    "                 dropout: float = 0.0,\n",
    "                 rbf_dmax: float = 3.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_convs = n_convs\n",
    "        self.in_scalar_size = in_scalar_size\n",
    "        self.out_scalar_size = out_scalar_size\n",
    "        self.vector_size = vector_size\n",
    "        self.dropout_rate = dropout\n",
    "        self.message_norm = message_norm\n",
    "        self.rbf_dmax = rbf_dmax\n",
    "\n",
    "        # check the message norm argument\n",
    "        if isinstance(message_norm, str) and message_norm != 'mean':\n",
    "            raise ValueError(f'message norm must be either a float, int, or \"mean\". Got {message_norm}')\n",
    "        elif isinstance(message_norm, float) or isinstance(message_norm, int):\n",
    "            pass\n",
    "        elif not isinstance(message_norm, (str, float, int)):\n",
    "            raise ValueError(f'message norm must be either a float, int, or \"mean\". Got {message_norm}')\n",
    "\n",
    "        # create functions to embed scalar features to the desired size\n",
    "        self.scalar_embed = nn.Sequential(\n",
    "            nn.Linear(in_scalar_size, out_scalar_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(out_scalar_size, out_scalar_size),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        self.scalar_norm = nn.LayerNorm(out_scalar_size)\n",
    "\n",
    "        edge_feat_size = 1\n",
    "\n",
    "        # create rec-rec convolutional layers\n",
    "        self.rec_conv_layers = nn.ModuleList()\n",
    "        for _ in range(n_convs):\n",
    "            self.rec_conv_layers.append(GVPEdgeConv(\n",
    "                scalar_size=out_scalar_size,\n",
    "                vector_size=vector_size,\n",
    "                n_message_gvps=n_message_gvps,\n",
    "                n_update_gvps=n_update_gvps,\n",
    "                edge_feat_size=edge_feat_size,\n",
    "                dropout=dropout,\n",
    "                message_norm=message_norm,\n",
    "                rbf_dmax=rbf_dmax\n",
    "            ))\n",
    "\n",
    "    def forward(self, g: dgl.DGLHeteroGraph):\n",
    "\n",
    "        device = g.device\n",
    "        batch_size = g.batch_size\n",
    "\n",
    "        # get scalar features\n",
    "        rec_scalar_feat = g.ndata[\"h_0\"]\n",
    "\n",
    "        # embed scalar features\n",
    "        rec_scalar_feat = self.scalar_embed(rec_scalar_feat)\n",
    "        rec_scalar_feat = self.scalar_norm(rec_scalar_feat)\n",
    "\n",
    "        # initialize receptor vector features\n",
    "        rec_vec_feat = torch.zeros((g.num_nodes(), self.vector_size, 3), device=device)\n",
    "\n",
    "        # get edge features\n",
    "        edge_feat = g.edata['a']\n",
    "\n",
    "        # get coordinate features\n",
    "        rec_coord_feat = g.ndata['x_0']\n",
    "\n",
    "        # compute the normalization factor for the messages if necessary\n",
    "        if self.message_norm == 'mean':\n",
    "            # set z to 1. the receptor convolutional layer will use mean aggregation instead of sum\n",
    "            z = 1\n",
    "        else:\n",
    "            z = self.message_norm\n",
    "\n",
    "        # apply receptor-receptor convolutions\n",
    "        for i in range(self.n_convs):\n",
    "            rec_feats = (rec_scalar_feat, rec_coord_feat, rec_vec_feat)\n",
    "            rec_scalar_feat, rec_vec_feat = self.rec_conv_layers[i](g, rec_feats=rec_feats, edge_feats=edge_feat, z=z)\n",
    "\n",
    "        vector_features_flattened = rec_vec_feat.view(rec_vec_feat.size(0), -1)  # Reshapes to [num_nodes, num_vectors * vector_dim]\n",
    "    \n",
    "        flattened_features = torch.cat([rec_scalar_feat, vector_features_flattened], dim=1)\n",
    "        graph_descriptor = torch.mean(flattened_features, dim=0)\n",
    "\n",
    "        return graph_descriptor\n",
    "    \n",
    "\n",
    "\n",
    "def con_loss(output1, output2, label, margin=1.0):\n",
    "    euclidean_distance = F.pairwise_distance(output1, output2)    \n",
    "    loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "                                (label) * torch.pow(torch.clamp(margin - euclidean_distance, min=0.0), 2))\n",
    "    return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup\n",
    "\n",
    "pocket_to_graph = GraphPocket()\n",
    "\n",
    "g = pocket_to_graph(pocket_path='../../TOUGH-M1/data/11asA/11asA_pocket.pdb')\n",
    "\n",
    "model = ReceptorEncoderGVP(\n",
    "    in_scalar_size=4,    #Matching the node_features dimension\n",
    "    out_scalar_size=128,  #Desired output scalar size\n",
    "    vector_size=16,       #Desired vector size\n",
    "    n_convs=3,            #Number of convolution layers\n",
    "    dropout=0.1           #Dropout rate\n",
    ")\n",
    "\n",
    "updated_g = model(g)\n",
    "\n",
    "print(\"Updated Features Shape:\", updated_g.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloader\n",
    "\n",
    "pos_path, neg_path = '../data/TOUGH-M1_positive.list', '../data/TOUGH-M1_negative.list'\n",
    "\n",
    "train_dataset, test_dataset = create_dataset(pos_path, neg_path, fold_nr=0, type='seq')\n",
    "train_dataloader = get_dataloader(train_dataset, batch_size=64, num_workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 2\n",
    "epoch_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    progress_bar = tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc=f'Epoch {epoch+1}/{epochs}')\n",
    "    \n",
    "    for batch_idx, ((graph1, graph2), label) in progress_bar:\n",
    "        graph1, graph2, label = graph1.to(device), graph2.to(device), label.to(device)\n",
    "        \n",
    "        output1 = model(graph1)\n",
    "        output2 = model(graph2)\n",
    "        \n",
    "        loss = con_loss(output1, output2, label)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        progress_bar.set_postfix({'Loss': running_loss/(batch_idx+1)})\n",
    "\n",
    "    avg_epoch_loss = running_loss / len(train_dataloader)\n",
    "    epoch_losses.append(avg_epoch_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
